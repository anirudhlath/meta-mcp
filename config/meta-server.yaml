# Meta MCP Server Configuration
# This file configures the Meta MCP Server with intelligent tool routing

server:
  name: "meta-mcp-server"
  host: "localhost"
  port: 3456

web_ui:
  enabled: true
  host: "localhost"
  port: 8080
  auth_enabled: false
  # Uncomment to enable authentication
  # username: "admin"
  # password: "${ADMIN_PASSWORD}"

strategy:
  primary: "vector"  # Options: vector, llm, rag
  fallback: "vector"
  vector_threshold: 0.4  # Lower threshold for better recall
  max_tools: 10

embeddings:
  # LM Studio configuration (primary)
  lm_studio_endpoint: "http://localhost:1234/v1/embeddings"
  lm_studio_model: "Nomic-embed-text-v1.5-Embedding-GGUF/nomic-embed-text-v1.5.f16.gguf"

  # Local fallback configuration
  fallback_model: "all-MiniLM-L6-v2"
  batch_size: 32
  cache_dir: "./embedding-models"

vector_store:
  type: "qdrant"
  host: "192.168.64.4"
  port: 6333
  collection_prefix: "meta_mcp"
  # Alternative: use full URL
  # url: "http://localhost:6333"

llm:
  endpoint: "http://localhost:1234/v1"
  model: "mistralai/magistral-small"
  temperature: 0.7
  max_tokens: 30000
  # api_key: "${LLM_API_KEY}"  # If required

rag:
  chunk_size: 500
  chunk_overlap: 50
  top_k: 5
  score_threshold: 0.7
  include_examples: true

# Child servers are now configured via separate JSON file
# Use --mcp-servers-json flag to specify the MCP servers configuration

logging:
  level: "INFO"
  file: "./logs/meta-server.log"
  max_files: 5
  max_size_mb: 10
  console: true
